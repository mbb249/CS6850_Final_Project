{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting graph preprocessing...\n",
      "Loading original features and edges...\n",
      "Loaded 30075 nodes with features.\n",
      "Loaded 701311 edges.\n",
      "Creating graph from edges...\n",
      "Graph created with 114324 nodes and 701311 edges.\n",
      "Identifying the largest connected component...\n",
      "Largest connected component has 111251 nodes and 699461 edges.\n",
      "Assigning features to nodes...\n",
      "Node features assigned.\n",
      "Standardizing features to fixed dimensions...\n",
      "Number of nodes with features: 2847\n",
      "Initial feature matrix shape: (2847, 10141)\n",
      "Reducing dimensions to 128 using PCA...\n",
      "Feature matrix shape after PCA: (2847, 128)\n",
      "Assigning standardized features back to nodes...\n",
      "Feature standardization complete.\n",
      "Graph preprocessing complete. Final graph has 111251 nodes and 699461 edges.\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Preprocessing and Graph Construction\n",
    "\n",
    "# File paths\n",
    "original_features_path = \"../foursquare_data/features.json\"\n",
    "original_edges_path = \"../foursquare_data/edges.csv\"\n",
    "\n",
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading original features and edges...\")\n",
    "    with open(original_features_path, \"r\") as f:\n",
    "        features = json.load(f)\n",
    "    print(f\"Loaded {len(features)} nodes with features.\")\n",
    "    \n",
    "    edges = pd.read_csv(\n",
    "        original_edges_path, names=[\"source\", \"target\"], skiprows=1\n",
    "    )  # Skip header row\n",
    "    print(f\"Loaded {len(edges)} edges.\")\n",
    "    return features, edges\n",
    "\n",
    "# Create graph and extract largest connected component\n",
    "def create_graph(features, edges):\n",
    "    print(\"Creating graph from edges...\")\n",
    "    edges[\"source\"] = edges[\"source\"].astype(str)\n",
    "    edges[\"target\"] = edges[\"target\"].astype(str)\n",
    "    G = nx.from_pandas_edgelist(edges, source=\"source\", target=\"target\")\n",
    "    print(f\"Graph created with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "\n",
    "    print(\"Identifying the largest connected component...\")\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_lcc = G.subgraph(largest_cc).copy()\n",
    "    print(f\"Largest connected component has {len(G_lcc.nodes)} nodes and {len(G_lcc.edges)} edges.\")\n",
    "\n",
    "    print(\"Assigning features to nodes...\")\n",
    "    for node, feats in features.items():\n",
    "        if node in G_lcc.nodes:\n",
    "            G_lcc.nodes[node][\"features\"] = feats\n",
    "    print(\"Node features assigned.\")\n",
    "    \n",
    "    return G_lcc\n",
    "\n",
    "def standardize_features(G, output_dim=128):\n",
    "    print(\"Standardizing features to fixed dimensions...\")\n",
    "\n",
    "    # Extract nodes with features\n",
    "    nodes_with_features = [\n",
    "        node for node, feats in nx.get_node_attributes(G, \"features\").items() if feats\n",
    "    ]\n",
    "    print(f\"Number of nodes with features: {len(nodes_with_features)}\")\n",
    "\n",
    "    # Extract feature lists for nodes with features\n",
    "    feature_list = [\n",
    "        set(G.nodes[node][\"features\"]) for node in nodes_with_features\n",
    "    ]\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    binary_features = mlb.fit_transform(feature_list)\n",
    "    print(f\"Initial feature matrix shape: {binary_features.shape}\")\n",
    "\n",
    "    # Apply PCA if necessary\n",
    "    if binary_features.shape[1] > output_dim:\n",
    "        print(f\"Reducing dimensions to {output_dim} using PCA...\")\n",
    "        pca = PCA(n_components=output_dim)\n",
    "        reduced_features = pca.fit_transform(binary_features)\n",
    "        print(f\"Feature matrix shape after PCA: {reduced_features.shape}\")\n",
    "    else:\n",
    "        print(f\"No dimensionality reduction needed. Retaining shape {binary_features.shape}\")\n",
    "        reduced_features = binary_features\n",
    "\n",
    "    # Assign standardized features back to the corresponding nodes\n",
    "    print(\"Assigning standardized features back to nodes...\")\n",
    "    for idx, node in enumerate(nodes_with_features):\n",
    "        G.nodes[node][\"features\"] = reduced_features[idx]\n",
    "    \n",
    "    print(\"Feature standardization complete.\")\n",
    "\n",
    "\n",
    "\n",
    "# Load, process, and standardize graph\n",
    "print(\"Starting graph preprocessing...\")\n",
    "features, edges = load_data()\n",
    "G = create_graph(features, edges)\n",
    "standardize_features(G, output_dim=128)\n",
    "print(f\"Graph preprocessing complete. Final graph has {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing BFS to ensure all nodes are connected...\n",
      "All nodes are connected. The graph is a single connected component.\n"
     ]
    }
   ],
   "source": [
    "def check_connectivity_bfs(G):\n",
    "    print(\"Performing BFS to ensure all nodes are connected...\")\n",
    "    start_node = next(iter(G.nodes))  # Get an arbitrary starting node\n",
    "    visited = set()\n",
    "    queue = [start_node]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            queue.extend(neighbor for neighbor in G.neighbors(node) if neighbor not in visited)\n",
    "\n",
    "    if len(visited) == len(G.nodes):\n",
    "        print(\"All nodes are connected. The graph is a single connected component.\")\n",
    "    else:\n",
    "        print(f\"Graph is not fully connected. Only {len(visited)} out of {len(G.nodes)} nodes are reachable.\")\n",
    "\n",
    "\n",
    "check_connectivity_bfs(G)  # Ensure the graph is a single connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and splitting feature vectors...\n",
      "Creating feature vectors for ML tasks...\n",
      "Processing positive samples (existing edges)...\n",
      "Processed 0 positive samples.\n",
      "Processed 50000 positive samples.\n",
      "Processed 100000 positive samples.\n",
      "Processed 150000 positive samples.\n",
      "Processed 200000 positive samples.\n",
      "Processed 250000 positive samples.\n",
      "Processed 300000 positive samples.\n",
      "Processed 350000 positive samples.\n",
      "Processed 400000 positive samples.\n",
      "Processed 450000 positive samples.\n",
      "Processed 500000 positive samples.\n",
      "Processed 550000 positive samples.\n",
      "Processed 600000 positive samples.\n",
      "Processed 650000 positive samples.\n",
      "Processed 700000 positive samples.\n",
      "Generating negative samples (random non-existing edges)...\n",
      "Generated 0 negative samples.\n",
      "Generated 50000 negative samples.\n",
      "Generated 100000 negative samples.\n",
      "Generated 150000 negative samples.\n",
      "Generated 200000 negative samples.\n",
      "Generated 250000 negative samples.\n",
      "Generated 300000 negative samples.\n",
      "Generated 350000 negative samples.\n",
      "Generated 400000 negative samples.\n",
      "Generated 450000 negative samples.\n",
      "Generated 500000 negative samples.\n",
      "Generated 550000 negative samples.\n",
      "Generated 600000 negative samples.\n",
      "Generated 650000 negative samples.\n",
      "Generated 700000 negative samples.\n",
      "Feature vectors created. Total samples: 706863\n",
      "Training set size: 565490, Test set size: 141373\n",
      "Feature vectors and splits saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Section 3A: Create Feature Vectors\n",
    "\n",
    "def create_feature_vectors(G, edges):\n",
    "    print(\"Creating feature vectors for ML tasks...\")\n",
    "    X, y = [], []\n",
    "\n",
    "    print(\"Processing positive samples (existing edges)...\")\n",
    "    for i, (_, row) in enumerate(edges.iterrows()):\n",
    "        node1, node2 = str(row[\"source\"]), str(row[\"target\"])\n",
    "        if (\n",
    "            node1 in G.nodes \n",
    "            and node2 in G.nodes \n",
    "            and \"features\" in G.nodes[node1] \n",
    "            and \"features\" in G.nodes[node2]\n",
    "        ):\n",
    "            feature_vector = np.array(G.nodes[node1][\"features\"]) - np.array(G.nodes[node2][\"features\"])\n",
    "            X.append(feature_vector)\n",
    "            y.append(1)\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"Processed {i} positive samples.\")\n",
    "\n",
    "    print(\"Generating negative samples (random non-existing edges)...\")\n",
    "    all_nodes = [node for node in G.nodes if \"features\" in G.nodes[node]]  # Nodes with features\n",
    "    for i in range(len(edges)):\n",
    "        node1, node2 = np.random.choice(all_nodes, 2, replace=False)\n",
    "        if not G.has_edge(node1, node2):\n",
    "            feature_vector = np.array(G.nodes[node1][\"features\"]) - np.array(G.nodes[node2][\"features\"])\n",
    "            X.append(feature_vector)\n",
    "            y.append(0)\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"Generated {i} negative samples.\")\n",
    "\n",
    "    print(f\"Feature vectors created. Total samples: {len(X)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Create and split feature vectors\n",
    "print(\"Creating and splitting feature vectors...\")\n",
    "X, y = create_feature_vectors(G, edges)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "with open(\"feature_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X, y, X_train, X_test, y_train, y_test), f)\n",
    "\n",
    "print(\"Feature vectors and splits saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors and splits loaded successfully.\n",
      "Training set size: 565490, Test set size: 141373\n"
     ]
    }
   ],
   "source": [
    "with open(\"feature_vectors.pkl\", \"rb\") as f:\n",
    "    X, y, X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "print(\"Feature vectors and splits loaded successfully.\")\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the neural network model...\n",
      "Model defined successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesbramwit/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Model compiled successfully.\n",
      "Starting model training...\n",
      "\n",
      "Epoch 1:\n",
      "  Training Loss: 0.0552, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0539, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 2:\n",
      "  Training Loss: 0.0515, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0529, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 3:\n",
      "  Training Loss: 0.0499, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0524, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 4:\n",
      "  Training Loss: 0.0484, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0527, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 5:\n",
      "  Training Loss: 0.0471, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0528, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 6:\n",
      "  Training Loss: 0.0459, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0535, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 7:\n",
      "  Training Loss: 0.0447, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0552, Validation Accuracy: 0.9901\n",
      "\n",
      "Epoch 8:\n",
      "  Training Loss: 0.0435, Training Accuracy: 0.9905\n",
      "  Validation Loss: 0.0554, Validation Accuracy: 0.9901\n",
      "\n",
      "Epoch 9:\n",
      "  Training Loss: 0.0423, Training Accuracy: 0.9905\n",
      "  Validation Loss: 0.0576, Validation Accuracy: 0.9900\n",
      "\n",
      "Epoch 10:\n",
      "  Training Loss: 0.0412, Training Accuracy: 0.9908\n",
      "  Validation Loss: 0.0589, Validation Accuracy: 0.9900\n",
      "\n",
      "Epoch 11:\n",
      "  Training Loss: 0.0402, Training Accuracy: 0.9909\n",
      "  Validation Loss: 0.0598, Validation Accuracy: 0.9897\n",
      "\n",
      "Epoch 12:\n",
      "  Training Loss: 0.0393, Training Accuracy: 0.9911\n",
      "  Validation Loss: 0.0629, Validation Accuracy: 0.9898\n",
      "\n",
      "Epoch 13:\n",
      "  Training Loss: 0.0383, Training Accuracy: 0.9913\n",
      "  Validation Loss: 0.0661, Validation Accuracy: 0.9897\n",
      "\n",
      "Epoch 14:\n",
      "  Training Loss: 0.0376, Training Accuracy: 0.9914\n",
      "  Validation Loss: 0.0689, Validation Accuracy: 0.9895\n",
      "\n",
      "Epoch 15:\n",
      "  Training Loss: 0.0370, Training Accuracy: 0.9915\n",
      "  Validation Loss: 0.0657, Validation Accuracy: 0.9899\n",
      "\n",
      "Epoch 16:\n",
      "  Training Loss: 0.0363, Training Accuracy: 0.9916\n",
      "  Validation Loss: 0.0665, Validation Accuracy: 0.9890\n",
      "\n",
      "Epoch 17:\n",
      "  Training Loss: 0.0358, Training Accuracy: 0.9917\n",
      "  Validation Loss: 0.0707, Validation Accuracy: 0.9891\n",
      "\n",
      "Epoch 18:\n",
      "  Training Loss: 0.0352, Training Accuracy: 0.9917\n",
      "  Validation Loss: 0.0792, Validation Accuracy: 0.9893\n",
      "\n",
      "Epoch 19:\n",
      "  Training Loss: 0.0348, Training Accuracy: 0.9920\n",
      "  Validation Loss: 0.0735, Validation Accuracy: 0.9891\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "  Training Loss: 0.0344, Training Accuracy: 0.9920\n",
      "  Validation Loss: 0.0765, Validation Accuracy: 0.9896\n",
      "Model training complete.\n",
      "Model saved successfully to 'trained_model.h5'.\n",
      "Training history saved successfully to 'training_history.pkl'.\n",
      "Evaluating the model on the test set...\n",
      "\u001b[1m4418/4418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 258us/step - accuracy: 0.9905 - loss: 0.0679\n",
      "Test Loss: 0.0714\n",
      "Test Accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "# Section 3B: Train the Neural Network\n",
    "\n",
    "# Define the neural network\n",
    "print(\"Defining the neural network model...\")\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],), name=\"Input_Layer\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\", name=\"Hidden_Layer_1\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", name=\"Hidden_Layer_2\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"Output_Layer\"),\n",
    "])\n",
    "print(\"Model defined successfully.\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "print(\"Compiling the model...\")\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"Model compiled successfully.\")\n",
    "\n",
    "# Define a custom callback for logging\n",
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nEpoch {epoch + 1}:\")\n",
    "        print(\n",
    "            f\"  Training Loss: {logs['loss']:.4f}, Training Accuracy: {logs['accuracy']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Validation Loss: {logs['val_loss']:.4f}, Validation Accuracy: {logs['val_accuracy']:.4f}\"\n",
    "        )\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[TrainingLogger()],\n",
    "    verbose=0  # Suppress default verbose to use custom logging\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.save(\"trained_model.h5\")\n",
    "print(\"Model saved successfully to 'trained_model.h5'.\")\n",
    "\n",
    "# Save the training history\n",
    "with open(\"training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved successfully to 'training_history.pkl'.\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from 'trained_model.h5'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,883</span> (105.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,883\u001b[0m (105.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history loaded successfully from 'training_history.pkl'.\n",
      "Loaded Training History:\n",
      "accuracy: [0.9903976917266846, 0.9904308915138245, 0.990428626537323, 0.9904198050498962, 0.990428626537323]...\n",
      "loss: [0.05517600476741791, 0.051534269005060196, 0.0499231219291687, 0.04841449856758118, 0.0470704585313797]...\n",
      "val_accuracy: [0.9901943206787109, 0.9901943206787109, 0.9901943206787109, 0.9901943206787109, 0.9901677966117859]...\n",
      "val_loss: [0.05393171310424805, 0.05294496938586235, 0.05241876840591431, 0.05273481085896492, 0.05276070907711983]...\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
    "print(\"Model loaded successfully from 'trained_model.h5'.\")\n",
    "model.summary()\n",
    "\n",
    "# Load the training history\n",
    "with open(\"training_history.pkl\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Training history loaded successfully from 'training_history.pkl'.\")\n",
    "\n",
    "# Print the loaded training history (optional)\n",
    "print(\"Loaded Training History:\")\n",
    "for key, values in history.items():\n",
    "    print(f\"{key}: {values[:5]}...\")  # Show first 5 values as a preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to find the best nodes\n",
    "def find_best_nodes(G, neighbors, target_node):\n",
    "    \"\"\"\n",
    "    Find the best nodes among neighbors based on shortest path distance to the target.\n",
    "    Returns the list of tied 'best' nodes with the minimum distance.\n",
    "    \"\"\"\n",
    "    distances = {\n",
    "        neighbor: nx.shortest_path_length(G, source=neighbor, target=target_node)\n",
    "        for neighbor in neighbors\n",
    "    }\n",
    "    min_distance = min(distances.values())\n",
    "    best_nodes = [node for node, dist in distances.items() if dist == min_distance]\n",
    "    return best_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_node(model, G, current_node, target_node, visited, prediction_cache, debug=False):\n",
    "    \"\"\"\n",
    "    Predict the next node from the current node and return step-by-step accuracy.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Predicting next node from current node: {current_node}\")\n",
    "\n",
    "    neighbors = [\n",
    "        neighbor for neighbor in G.neighbors(current_node)\n",
    "        if \"features\" in G.nodes[neighbor] and neighbor not in visited\n",
    "    ]\n",
    "\n",
    "    if not neighbors:\n",
    "        random_choice = random.choice(list(G.neighbors(current_node))) if G.neighbors(current_node) else None\n",
    "        if debug:\n",
    "            print(\"Opting For Random Choice\")\n",
    "        return random_choice, False  # No \"best choice\" available, random node selected\n",
    "    \n",
    "    if target_node in neighbors:\n",
    "        if debug:\n",
    "            print(f\"Target node {target_node} is a direct neighbor. Automatically selecting it.\")\n",
    "        return target_node, True  # Automatically move to the destination and mark as accurate\n",
    "\n",
    "    # Find the best nodes\n",
    "    best_nodes = find_best_nodes(G, neighbors, target_node)\n",
    "\n",
    "    # Predict probabilities for neighbors\n",
    "    predictions = []\n",
    "    for neighbor in neighbors:\n",
    "        feature_vector = G.nodes[neighbor][\"features\"] - G.nodes[target_node][\"features\"]\n",
    "        prob = model.predict(feature_vector.reshape(1, -1), verbose=0)[0][0]\n",
    "        predictions.append((neighbor, prob))\n",
    "\n",
    "    # Sort by probability\n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Neighbors Were Available, Chance of Containing Edge:{predictions[0][1]}, Selected Node {predictions[0][0]}\" )\n",
    "    chosen_node = predictions[0][0]  # Node with the highest probability\n",
    "\n",
    "    is_accurate = chosen_node in best_nodes\n",
    "    return chosen_node, is_accurate\n",
    "\n",
    "def find_path(model, G, source, target, max_hops=40, debug=False):\n",
    "    \"\"\"\n",
    "    Find the path while tracking accuracy at each step.\n",
    "    Outputs path and per-move accuracy data.\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Starting pathfinding from source: {source} to target: {target}, with max hops: {max_hops}\")\n",
    "\n",
    "    current_node = source\n",
    "    visited = set()\n",
    "    prediction_cache = {}\n",
    "    path = [source]  # Track path regardless of success\n",
    "    hops = 0\n",
    "    correct_choices = 0\n",
    "    step_actuals = []  # Ground truth: 1 when a \"best choice\" exists\n",
    "    step_predictions = []  # Prediction: 1 if chosen move is among the best\n",
    "\n",
    "    while hops < max_hops:\n",
    "        if current_node == target:  # Success condition\n",
    "            if debug:\n",
    "                print(f\"Pathfinding succeeded: target {target} reached.\")\n",
    "            return path, correct_choices, hops, step_actuals, step_predictions\n",
    "\n",
    "        visited.add(current_node)\n",
    "        next_node, is_accurate = predict_next_node(model, G, current_node, target, visited, prediction_cache, debug=debug)\n",
    "\n",
    "        if not next_node:  # No valid moves left\n",
    "            if debug:\n",
    "                print(f\"Pathfinding failed: no valid neighbors from {current_node}.\")\n",
    "            return path, correct_choices, hops, step_actuals, step_predictions\n",
    "\n",
    "        # Log step accuracy\n",
    "        step_actuals.append(1)  # A \"best node\" always exists\n",
    "        step_predictions.append(1 if is_accurate else 0)\n",
    "\n",
    "        if is_accurate:\n",
    "            correct_choices += 1\n",
    "\n",
    "        # Update path and state\n",
    "        path.append(next_node)\n",
    "        current_node = next_node\n",
    "        hops += 1\n",
    "\n",
    "    # Failure due to exceeding max hops\n",
    "    if debug:\n",
    "        print(f\"Pathfinding failed: exceeded max hops ({max_hops}).\")\n",
    "    return path, correct_choices, hops, step_actuals, step_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Run 1/100: Source 11780 -> Target 28564\n",
      "  Path: ['11780', '9984', '9935', '12233', '10648', '12823', '7943', '12144', '13352', '26396', '10175', '18102', '12832', '10849', '11071', '10994', '15110', '12396', '16399', '12350', '13006']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 2/100: Source 2698 -> Target 15861\n",
      "  Path: ['2698', '9917', '1668', '54', '528', '16637', '758', '541', '19495', '854', '953', '866', '5975', '8524', '4786', '12184', '6034', '27138', '3812', '6571', '15890']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 3/100: Source 28524 -> Target 10373\n",
      "  Path: ['28524', '19785', '7199', '541', '24306', '20962', '78', '10716', '9041', '12645', '534', '545', '23897', '12555', '6158', '26417', '11717', '28916', '26863', '23321', '1536']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 4/100: Source 18431 -> Target 13072\n",
      "  Path: ['18431', '8825', '258282', '8825', '18431', '316587', '530738', '112489', '1110031', '112489', '358514', '178698', '29967', '14571', '20902', '20163', '13708', '20163', '9657', '2907', '20521']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 5/100: Source 4786 -> Target 20633\n",
      "  Path: ['4786', '8524', '3339', '22951', '16410', '4076', '16462', '12428', '9923', '14072', '3849', '6702', '24481', '6891', '9072', '26555', '11132', '12786', '10034', '801', '806']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 6/100: Source 10425 -> Target 16220\n",
      "  Path: ['10425', '806', '801', '12368', '1488', '1536', '3307', '4597', '3849', '3339', '20839', '10189', '8155', '6891', '9072', '19102', '12497', '649', '12062', '6928', '9418']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 7/100: Source 24425 -> Target 14784\n",
      "  Path: ['24425', '27544', '12780', '29889', '26403', '24720', '17476', '541', '6347', '25018', '4786', '8524', '5975', '295', '535', '4024', '1625', '3131', '1723', '1877', '7759']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 8/100: Source 15976 -> Target 15877\n",
      "  Path: ['15976', '3365', '3924', '5334', '20683', '16462', '3351', '9978', '9923', '4076', '3306', '4597', '12062', '649', '7841', '12786', '13585', '10871', '18040', '26863', '10034']\n",
      "  Path failed after 20 steps.\n",
      "\n",
      "Run 9/100: Source 3275 -> Target 16348\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def evaluate_pathfinding(model, G, max_hops=20, num_runs=20):\n",
    "    \"\"\"\n",
    "    Evaluate the pathfinding algorithm, calculate metrics, and print paths.\n",
    "    \"\"\"\n",
    "    total_hops = 0\n",
    "    successful_runs = 0\n",
    "    total_correct_choices = 0\n",
    "    total_steps = 0\n",
    "    all_actuals = []  # Tracks actual best node occurrences\n",
    "    all_predictions = []  # Tracks predictions (1 if accurate, 0 otherwise)\n",
    "\n",
    "    nodes_with_features = [node for node in G.nodes if \"features\" in G.nodes[node]]\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        if len(nodes_with_features) < 2:\n",
    "            print(\"Not enough nodes with features for evaluation.\")\n",
    "            break\n",
    "\n",
    "        source_node, target_node = random.sample(nodes_with_features, 2)\n",
    "        print(f\"\\nRun {run + 1}/{num_runs}: Source {source_node} -> Target {target_node}\")\n",
    "\n",
    "        # Run pathfinding\n",
    "        path, correct_choices, steps, step_actuals, step_predictions = find_path(model, G, source_node, target_node, max_hops)\n",
    "\n",
    "        # Print path regardless of success\n",
    "        print(f\"  Path: {path}\")\n",
    "\n",
    "        if path and path[-1] == target_node:\n",
    "            print(f\"  Path found in {steps} steps. Correct choices: {correct_choices}/{steps}\")\n",
    "            successful_runs += 1\n",
    "            total_hops += steps\n",
    "        else:\n",
    "            print(f\"  Path failed after {steps} steps.\")\n",
    "\n",
    "        # Update precision/recall data\n",
    "        all_actuals.extend(step_actuals)\n",
    "        all_predictions.extend(step_predictions)\n",
    "\n",
    "        # Track total moves\n",
    "        total_correct_choices += correct_choices\n",
    "        total_steps += steps\n",
    "\n",
    "    # Metrics\n",
    "    success_rate = successful_runs / num_runs * 100\n",
    "    average_hops = total_hops / successful_runs if successful_runs > 0 else float('inf')\n",
    "    accuracy = total_correct_choices / total_steps if total_steps > 0 else 0.0\n",
    "    f1 = f1_score(all_actuals, all_predictions, zero_division=0)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Success rate: {success_rate:.2f}% ({successful_runs}/{num_runs})\")\n",
    "    print(f\"Average hops: {average_hops:.2f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"F1-score: {f1:.2f}\")\n",
    "\n",
    "    return success_rate, average_hops, accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "success_rate, average_hops, accuracy, precision, recall, f1 = evaluate_pathfinding(model, G, max_hops=20, num_runs=100)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
