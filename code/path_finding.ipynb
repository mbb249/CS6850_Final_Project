{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 2: Preprocessing and Graph Construction\n",
    "\n",
    "# File paths\n",
    "original_features_path = \"../git_web_ml/musae_git_features.json\"\n",
    "original_edges_path = \"../git_web_ml/musae_git_edges.csv\"\n",
    "\n",
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading original features and edges...\")\n",
    "    with open(original_features_path, \"r\") as f:\n",
    "        features = json.load(f)\n",
    "    print(f\"Loaded {len(features)} nodes with features.\")\n",
    "    \n",
    "    edges = pd.read_csv(\n",
    "        original_edges_path, names=[\"source\", \"target\"], skiprows=1\n",
    "    )  # Skip header row\n",
    "    print(f\"Loaded {len(edges)} edges.\")\n",
    "    return features, edges\n",
    "\n",
    "# Create graph and extract largest connected component\n",
    "def create_graph(features, edges):\n",
    "    print(\"Creating graph from edges...\")\n",
    "    edges[\"source\"] = edges[\"source\"].astype(str)\n",
    "    edges[\"target\"] = edges[\"target\"].astype(str)\n",
    "    G = nx.from_pandas_edgelist(edges, source=\"source\", target=\"target\")\n",
    "    print(f\"Graph created with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "\n",
    "    print(\"Identifying the largest connected component...\")\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_lcc = G.subgraph(largest_cc).copy()\n",
    "    print(f\"Largest connected component has {len(G_lcc.nodes)} nodes and {len(G_lcc.edges)} edges.\")\n",
    "\n",
    "    print(\"Assigning features to nodes...\")\n",
    "    for node, feats in features.items():\n",
    "        if node in G_lcc.nodes:\n",
    "            G_lcc.nodes[node][\"features\"] = feats\n",
    "    print(\"Node features assigned.\")\n",
    "    \n",
    "    return G_lcc\n",
    "\n",
    "# Standardize features using MultiLabelBinarizer and PCA\n",
    "def standardize_features(G, output_dim=128):\n",
    "    print(\"Standardizing features to fixed dimensions...\")\n",
    "    feature_list = [\n",
    "        set(feats) for feats in nx.get_node_attributes(G, \"features\").values()\n",
    "    ]\n",
    "    mlb = MultiLabelBinarizer()\n",
    "    binary_features = mlb.fit_transform(feature_list)\n",
    "    print(f\"Initial feature matrix shape: {binary_features.shape}\")\n",
    "\n",
    "    if binary_features.shape[1] > output_dim:\n",
    "        print(f\"Reducing dimensions to {output_dim} using PCA...\")\n",
    "        pca = PCA(n_components=output_dim)\n",
    "        reduced_features = pca.fit_transform(binary_features)\n",
    "        print(f\"Feature matrix shape after PCA: {reduced_features.shape}\")\n",
    "    else:\n",
    "        print(f\"No dimensionality reduction needed. Retaining shape {binary_features.shape}\")\n",
    "        reduced_features = binary_features\n",
    "\n",
    "    print(\"Assigning standardized features back to nodes...\")\n",
    "    for idx, node in enumerate(G.nodes):\n",
    "        G.nodes[node][\"features\"] = reduced_features[idx]\n",
    "    print(\"Feature standardization complete.\")\n",
    "\n",
    "# Load, process, and standardize graph\n",
    "print(\"Starting graph preprocessing...\")\n",
    "features, edges = load_data()\n",
    "G = create_graph(features, edges)\n",
    "standardize_features(G, output_dim=128)\n",
    "print(f\"Graph preprocessing complete. Final graph has {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_connectivity_bfs(G):\n",
    "    print(\"Performing BFS to ensure all nodes are connected...\")\n",
    "    start_node = next(iter(G.nodes))  # Get an arbitrary starting node\n",
    "    visited = set()\n",
    "    queue = [start_node]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            queue.extend(neighbor for neighbor in G.neighbors(node) if neighbor not in visited)\n",
    "\n",
    "    if len(visited) == len(G.nodes):\n",
    "        print(\"All nodes are connected. The graph is a single connected component.\")\n",
    "    else:\n",
    "        print(f\"Graph is not fully connected. Only {len(visited)} out of {len(G.nodes)} nodes are reachable.\")\n",
    "\n",
    "\n",
    "check_connectivity_bfs(G)  # Ensure the graph is a single connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3A: Create Feature Vectors\n",
    "\n",
    "def create_feature_vectors(G, edges):\n",
    "    print(\"Creating feature vectors for ML tasks...\")\n",
    "    X, y = [], []\n",
    "\n",
    "    print(\"Processing positive samples (existing edges)...\")\n",
    "    for i, (_, row) in enumerate(edges.iterrows()):\n",
    "        node1, node2 = str(row[\"source\"]), str(row[\"target\"])\n",
    "        if node1 in G.nodes and node2 in G.nodes:\n",
    "            feature_vector = np.array(G.nodes[node1][\"features\"]) - np.array(G.nodes[node2][\"features\"])\n",
    "            X.append(feature_vector)\n",
    "            y.append(1)\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"Processed {i} positive samples.\")\n",
    "\n",
    "    print(\"Generating negative samples (random non-existing edges)...\")\n",
    "    all_nodes = list(G.nodes)\n",
    "    for i in range(len(edges)):\n",
    "        node1, node2 = np.random.choice(all_nodes, 2, replace=False)\n",
    "        if not G.has_edge(node1, node2):\n",
    "            feature_vector = np.array(G.nodes[node1][\"features\"]) - np.array(G.nodes[node2][\"features\"])\n",
    "            X.append(feature_vector)\n",
    "            y.append(0)\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"Generated {i} negative samples.\")\n",
    "\n",
    "    print(f\"Feature vectors created. Total samples: {len(X)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Create and split feature vectors\n",
    "print(\"Creating and splitting feature vectors...\")\n",
    "X, y = create_feature_vectors(G, edges)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "with open(\"feature_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X, y, X_train, X_test, y_train, y_test), f)\n",
    "\n",
    "print(\"Feature vectors and splits saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"feature_vectors.pkl\", \"rb\") as f:\n",
    "    X, y, X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "print(\"Feature vectors and splits loaded successfully.\")\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 3B: Train the Neural Network\n",
    "\n",
    "# Define the neural network\n",
    "print(\"Defining the neural network model...\")\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],), name=\"Input_Layer\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\", name=\"Hidden_Layer_1\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", name=\"Hidden_Layer_2\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"Output_Layer\"),\n",
    "])\n",
    "print(\"Model defined successfully.\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "print(\"Compiling the model...\")\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"Model compiled successfully.\")\n",
    "\n",
    "# Define a custom callback for logging\n",
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nEpoch {epoch + 1}:\")\n",
    "        print(\n",
    "            f\"  Training Loss: {logs['loss']:.4f}, Training Accuracy: {logs['accuracy']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Validation Loss: {logs['val_loss']:.4f}, Validation Accuracy: {logs['val_accuracy']:.4f}\"\n",
    "        )\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[TrainingLogger()],\n",
    "    verbose=0  # Suppress default verbose to use custom logging\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.save(\"trained_model.h5\")\n",
    "print(\"Model saved successfully to 'trained_model.h5'.\")\n",
    "\n",
    "# Save the training history\n",
    "with open(\"training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved successfully to 'training_history.pkl'.\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
    "print(\"Model loaded successfully from 'trained_model.h5'.\")\n",
    "model.summary()\n",
    "\n",
    "# Load the training history\n",
    "with open(\"training_history.pkl\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Training history loaded successfully from 'training_history.pkl'.\")\n",
    "\n",
    "# Print the loaded training history (optional)\n",
    "print(\"Loaded Training History:\")\n",
    "for key, values in history.items():\n",
    "    print(f\"{key}: {values[:5]}...\")  # Show first 5 values as a preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_node(model, G, current_node, target_node, visited, prediction_cache, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Predicting next node from current node: {current_node}, target: {target_node}\")\n",
    "    \n",
    "    neighbors = [n for n in G.neighbors(current_node) if n not in visited]\n",
    "    \n",
    "    if not neighbors:\n",
    "        if debug:\n",
    "            print(\"No unvisited neighbors available.\")\n",
    "        return None  # No unvisited neighbors\n",
    "\n",
    "    # Check if the target node is one of the neighbors\n",
    "    if target_node in neighbors:\n",
    "        if debug:\n",
    "            print(f\"Target node {target_node} is a direct neighbor of {current_node}. Auto-selecting target.\")\n",
    "        return target_node  # Auto-select the target node\n",
    "\n",
    "    # Cache predictions to avoid redundant computations\n",
    "    if current_node not in prediction_cache:\n",
    "        target_features = G.nodes[target_node][\"features\"]\n",
    "        predictions = []\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_features = G.nodes[neighbor][\"features\"]\n",
    "            feature_vector = neighbor_features - target_features\n",
    "            prob = model.predict(feature_vector.reshape(1, -1), verbose=0)[0][0]  # Suppress model output\n",
    "            predictions.append((neighbor, prob))\n",
    "            if debug:\n",
    "                print(f\"Prediction for neighbor {neighbor}: {prob:.4f}\")\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        prediction_cache[current_node] = predictions\n",
    "    else:\n",
    "        if debug:\n",
    "            print(f\"Using cached predictions for {current_node}\")\n",
    "        predictions = prediction_cache[current_node]\n",
    "\n",
    "    # Select the next node based on predictions\n",
    "    for neighbor, prob in predictions:\n",
    "        if neighbor not in visited:\n",
    "            if debug:\n",
    "                print(f\"Next node selected: {neighbor} with probability {prob:.4f}\")\n",
    "            return neighbor\n",
    "\n",
    "    if debug:\n",
    "        print(\"No valid next node found.\")\n",
    "    return None\n",
    "\n",
    "\n",
    "# Find the path with a limit on the number of hops\n",
    "def find_path(model, G, source, target, max_hops=40, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Starting pathfinding from source: {source} to target: {target}, with max hops: {max_hops}\")\n",
    "    current_node = source\n",
    "    visited = set()\n",
    "    prediction_cache = {}  # Cache predictions to avoid recomputation\n",
    "    path = [source]\n",
    "    hops = 0\n",
    "\n",
    "    while current_node != target:\n",
    "        print(f\"Current Number of Hops: {hops}\")\n",
    "        print(f\"Current Node: {current_node}\")\n",
    "        visited.add(current_node)\n",
    "        next_node = predict_next_node(model, G, current_node, target, visited, prediction_cache, debug=debug)\n",
    "        if next_node is None:\n",
    "            if debug:\n",
    "                print(f\"Pathfinding failed: no valid neighbors from {current_node}.\")\n",
    "            return None  # No path found\n",
    "        path.append(next_node)\n",
    "        current_node = next_node\n",
    "        hops += 1\n",
    "\n",
    "        if hops > max_hops:\n",
    "            if debug:\n",
    "                print(f\"Pathfinding terminated: exceeded max hops ({max_hops}).\")\n",
    "            return None\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Pathfinding complete. Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate pathfinding with paths included in the output\n",
    "def evaluate_pathfinding(model, G, max_hops=20, num_runs=20):\n",
    "    total_hops = 0\n",
    "    successful_runs = 0\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        # Randomly choose start and target nodes\n",
    "        source_node, target_node = random.sample(list(G.nodes), 2)\n",
    "        print(f\"Run {run + 1}/{num_runs}: Source {source_node} -> Target {target_node}\")\n",
    "        \n",
    "        path = find_path(model, G, source_node, target_node, max_hops)\n",
    "        \n",
    "        if path:\n",
    "            num_hops = len(path) - 1  # Number of hops is the length of the path minus 1\n",
    "            print(f\"  Path found in {num_hops} hops. Path: {path}\")\n",
    "            total_hops += num_hops\n",
    "            successful_runs += 1\n",
    "        else:\n",
    "            print(\"  No path found or run terminated.\")\n",
    "    \n",
    "    if successful_runs > 0:\n",
    "        average_hops = total_hops / successful_runs\n",
    "        success_rate = successful_runs / num_runs * 100\n",
    "    else:\n",
    "        average_hops = float('inf')  # No successful runs\n",
    "        success_rate = 0.0\n",
    "    \n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Success rate: {success_rate:.2f}% ({successful_runs}/{num_runs})\")\n",
    "    print(f\"Average hops: {average_hops:.2f}\" if successful_runs > 0 else \"No successful runs.\")\n",
    "    \n",
    "    return success_rate, average_hops\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, average_hops = evaluate_pathfinding(model, G, max_hops=20, num_runs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
