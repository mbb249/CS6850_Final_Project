{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import networkx as nx\n",
    "import json\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "print(\"Libraries imported successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting graph preprocessing...\n",
      "Loading original features and edges...\n",
      "Loaded 30075 nodes with features.\n",
      "Loaded 701311 edges.\n",
      "Creating graph from edges...\n",
      "Graph created with 114324 nodes and 701311 edges.\n",
      "Identifying the largest connected component...\n",
      "Largest connected component has 111251 nodes and 699461 edges.\n",
      "Assigning features to nodes...\n",
      "Node features assigned.\n",
      "Standardizing features to fixed dimensions...\n",
      "Number of nodes with features: 2847\n",
      "Initial feature matrix shape: (2847, 10141)\n",
      "Reducing dimensions to 128 using PCA...\n",
      "Feature matrix shape after PCA: (2847, 128)\n",
      "Assigning standardized features back to nodes...\n",
      "Feature standardization complete.\n",
      "Graph preprocessing complete. Final graph has 111251 nodes and 699461 edges.\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Preprocessing and Graph Construction\n",
    "\n",
    "# File paths\n",
    "original_features_path = \"../foursquare_data/features.json\"\n",
    "original_edges_path = \"../foursquare_data/edges.csv\"\n",
    "\n",
    "# Load data\n",
    "def load_data():\n",
    "    print(\"Loading original features and edges...\")\n",
    "    with open(original_features_path, \"r\") as f:\n",
    "        features = json.load(f)\n",
    "    print(f\"Loaded {len(features)} nodes with features.\")\n",
    "    \n",
    "    edges = pd.read_csv(\n",
    "        original_edges_path, names=[\"source\", \"target\"], skiprows=1\n",
    "    )  # Skip header row\n",
    "    print(f\"Loaded {len(edges)} edges.\")\n",
    "    return features, edges\n",
    "\n",
    "# Create graph and extract largest connected component\n",
    "def create_graph(features, edges):\n",
    "    print(\"Creating graph from edges...\")\n",
    "    edges[\"source\"] = edges[\"source\"].astype(str)\n",
    "    edges[\"target\"] = edges[\"target\"].astype(str)\n",
    "    G = nx.from_pandas_edgelist(edges, source=\"source\", target=\"target\")\n",
    "    print(f\"Graph created with {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n",
    "\n",
    "    print(\"Identifying the largest connected component...\")\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    G_lcc = G.subgraph(largest_cc).copy()\n",
    "    print(f\"Largest connected component has {len(G_lcc.nodes)} nodes and {len(G_lcc.edges)} edges.\")\n",
    "\n",
    "    print(\"Assigning features to nodes...\")\n",
    "    for node, feats in features.items():\n",
    "        if node in G_lcc.nodes:\n",
    "            G_lcc.nodes[node][\"features\"] = feats\n",
    "    print(\"Node features assigned.\")\n",
    "    \n",
    "    return G_lcc\n",
    "\n",
    "def standardize_features(G, output_dim=128):\n",
    "    print(\"Standardizing features to fixed dimensions...\")\n",
    "\n",
    "    # Extract nodes with features\n",
    "    nodes_with_features = [\n",
    "        node for node, feats in nx.get_node_attributes(G, \"features\").items() if feats\n",
    "    ]\n",
    "    print(f\"Number of nodes with features: {len(nodes_with_features)}\")\n",
    "\n",
    "    # Extract feature lists for nodes with features\n",
    "    feature_list = [\n",
    "        set(G.nodes[node][\"features\"]) for node in nodes_with_features\n",
    "    ]\n",
    "    \n",
    "    mlb = MultiLabelBinarizer()\n",
    "    binary_features = mlb.fit_transform(feature_list)\n",
    "    print(f\"Initial feature matrix shape: {binary_features.shape}\")\n",
    "\n",
    "    # Apply PCA if necessary\n",
    "    if binary_features.shape[1] > output_dim:\n",
    "        print(f\"Reducing dimensions to {output_dim} using PCA...\")\n",
    "        pca = PCA(n_components=output_dim)\n",
    "        reduced_features = pca.fit_transform(binary_features)\n",
    "        print(f\"Feature matrix shape after PCA: {reduced_features.shape}\")\n",
    "    else:\n",
    "        print(f\"No dimensionality reduction needed. Retaining shape {binary_features.shape}\")\n",
    "        reduced_features = binary_features\n",
    "\n",
    "    # Assign standardized features back to the corresponding nodes\n",
    "    print(\"Assigning standardized features back to nodes...\")\n",
    "    for idx, node in enumerate(nodes_with_features):\n",
    "        G.nodes[node][\"features\"] = reduced_features[idx]\n",
    "    \n",
    "    print(\"Feature standardization complete.\")\n",
    "\n",
    "\n",
    "\n",
    "# Load, process, and standardize graph\n",
    "print(\"Starting graph preprocessing...\")\n",
    "features, edges = load_data()\n",
    "G = create_graph(features, edges)\n",
    "standardize_features(G, output_dim=128)\n",
    "print(f\"Graph preprocessing complete. Final graph has {len(G.nodes)} nodes and {len(G.edges)} edges.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing BFS to ensure all nodes are connected...\n",
      "All nodes are connected. The graph is a single connected component.\n"
     ]
    }
   ],
   "source": [
    "def check_connectivity_bfs(G):\n",
    "    print(\"Performing BFS to ensure all nodes are connected...\")\n",
    "    start_node = next(iter(G.nodes))  # Get an arbitrary starting node\n",
    "    visited = set()\n",
    "    queue = [start_node]\n",
    "\n",
    "    while queue:\n",
    "        node = queue.pop(0)\n",
    "        if node not in visited:\n",
    "            visited.add(node)\n",
    "            queue.extend(neighbor for neighbor in G.neighbors(node) if neighbor not in visited)\n",
    "\n",
    "    if len(visited) == len(G.nodes):\n",
    "        print(\"All nodes are connected. The graph is a single connected component.\")\n",
    "    else:\n",
    "        print(f\"Graph is not fully connected. Only {len(visited)} out of {len(G.nodes)} nodes are reachable.\")\n",
    "\n",
    "\n",
    "check_connectivity_bfs(G)  # Ensure the graph is a single connected component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating and splitting feature vectors...\n",
      "Creating feature vectors for ML tasks...\n",
      "Processing positive samples (existing edges)...\n",
      "Processed 0 positive samples.\n",
      "Processed 50000 positive samples.\n",
      "Processed 100000 positive samples.\n",
      "Processed 150000 positive samples.\n",
      "Processed 200000 positive samples.\n",
      "Processed 250000 positive samples.\n",
      "Processed 300000 positive samples.\n",
      "Processed 350000 positive samples.\n",
      "Processed 400000 positive samples.\n",
      "Processed 450000 positive samples.\n",
      "Processed 500000 positive samples.\n",
      "Processed 550000 positive samples.\n",
      "Processed 600000 positive samples.\n",
      "Processed 650000 positive samples.\n",
      "Processed 700000 positive samples.\n",
      "Generating negative samples (random non-existing edges)...\n",
      "Generated 0 negative samples.\n",
      "Generated 50000 negative samples.\n",
      "Generated 100000 negative samples.\n",
      "Generated 150000 negative samples.\n",
      "Generated 200000 negative samples.\n",
      "Generated 250000 negative samples.\n",
      "Generated 300000 negative samples.\n",
      "Generated 350000 negative samples.\n",
      "Generated 400000 negative samples.\n",
      "Generated 450000 negative samples.\n",
      "Generated 500000 negative samples.\n",
      "Generated 550000 negative samples.\n",
      "Generated 600000 negative samples.\n",
      "Generated 650000 negative samples.\n",
      "Generated 700000 negative samples.\n",
      "Feature vectors created. Total samples: 706865\n",
      "Training set size: 565492, Test set size: 141373\n",
      "Feature vectors and splits saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Section 3A: Create Feature Vectors\n",
    "\n",
    "def create_feature_vectors(G, edges):\n",
    "    print(\"Creating feature vectors for ML tasks...\")\n",
    "    X, y = [], []\n",
    "\n",
    "    print(\"Processing positive samples (existing edges)...\")\n",
    "    for i, (_, row) in enumerate(edges.iterrows()):\n",
    "        node1, node2 = str(row[\"source\"]), str(row[\"target\"])\n",
    "        if (\n",
    "            node1 in G.nodes \n",
    "            and node2 in G.nodes \n",
    "            and \"features\" in G.nodes[node1] \n",
    "            and \"features\" in G.nodes[node2]\n",
    "        ):\n",
    "            feature_vector = np.array(G.nodes[node1][\"features\"]) - np.array(G.nodes[node2][\"features\"])\n",
    "            X.append(feature_vector)\n",
    "            y.append(1)\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"Processed {i} positive samples.\")\n",
    "\n",
    "    print(\"Generating negative samples (random non-existing edges)...\")\n",
    "    all_nodes = [node for node in G.nodes if \"features\" in G.nodes[node]]  # Nodes with features\n",
    "    for i in range(len(edges)):\n",
    "        node1, node2 = np.random.choice(all_nodes, 2, replace=False)\n",
    "        if not G.has_edge(node1, node2):\n",
    "            feature_vector = np.array(G.nodes[node1][\"features\"]) - np.array(G.nodes[node2][\"features\"])\n",
    "            X.append(feature_vector)\n",
    "            y.append(0)\n",
    "        if i % 50000 == 0:\n",
    "            print(f\"Generated {i} negative samples.\")\n",
    "\n",
    "    print(f\"Feature vectors created. Total samples: {len(X)}\")\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "# Create and split feature vectors\n",
    "print(\"Creating and splitting feature vectors...\")\n",
    "X, y = create_feature_vectors(G, edges)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")\n",
    "\n",
    "with open(\"feature_vectors.pkl\", \"wb\") as f:\n",
    "    pickle.dump((X, y, X_train, X_test, y_train, y_test), f)\n",
    "\n",
    "print(\"Feature vectors and splits saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature vectors and splits loaded successfully.\n",
      "Training set size: 565492, Test set size: 141373\n"
     ]
    }
   ],
   "source": [
    "with open(\"feature_vectors.pkl\", \"rb\") as f:\n",
    "    X, y, X_train, X_test, y_train, y_test = pickle.load(f)\n",
    "\n",
    "print(\"Feature vectors and splits loaded successfully.\")\n",
    "print(f\"Training set size: {X_train.shape[0]}, Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defining the neural network model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/milesbramwit/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model defined successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling the model...\n",
      "Model compiled successfully.\n",
      "Starting model training...\n",
      "\n",
      "Epoch 1:\n",
      "  Training Loss: 0.0550, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0538, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 2:\n",
      "  Training Loss: 0.0515, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0530, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 3:\n",
      "  Training Loss: 0.0498, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0525, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 4:\n",
      "  Training Loss: 0.0483, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0525, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 5:\n",
      "  Training Loss: 0.0470, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0529, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 6:\n",
      "  Training Loss: 0.0458, Training Accuracy: 0.9904\n",
      "  Validation Loss: 0.0537, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 7:\n",
      "  Training Loss: 0.0446, Training Accuracy: 0.9905\n",
      "  Validation Loss: 0.0552, Validation Accuracy: 0.9902\n",
      "\n",
      "Epoch 8:\n",
      "  Training Loss: 0.0434, Training Accuracy: 0.9905\n",
      "  Validation Loss: 0.0561, Validation Accuracy: 0.9900\n",
      "\n",
      "Epoch 9:\n",
      "  Training Loss: 0.0422, Training Accuracy: 0.9906\n",
      "  Validation Loss: 0.0573, Validation Accuracy: 0.9901\n",
      "\n",
      "Epoch 10:\n",
      "  Training Loss: 0.0414, Training Accuracy: 0.9906\n",
      "  Validation Loss: 0.0581, Validation Accuracy: 0.9900\n",
      "\n",
      "Epoch 11:\n",
      "  Training Loss: 0.0403, Training Accuracy: 0.9908\n",
      "  Validation Loss: 0.0653, Validation Accuracy: 0.9899\n",
      "\n",
      "Epoch 12:\n",
      "  Training Loss: 0.0395, Training Accuracy: 0.9909\n",
      "  Validation Loss: 0.0670, Validation Accuracy: 0.9898\n",
      "\n",
      "Epoch 13:\n",
      "  Training Loss: 0.0386, Training Accuracy: 0.9910\n",
      "  Validation Loss: 0.0638, Validation Accuracy: 0.9892\n",
      "\n",
      "Epoch 14:\n",
      "  Training Loss: 0.0378, Training Accuracy: 0.9912\n",
      "  Validation Loss: 0.0699, Validation Accuracy: 0.9890\n",
      "\n",
      "Epoch 15:\n",
      "  Training Loss: 0.0370, Training Accuracy: 0.9914\n",
      "  Validation Loss: 0.0781, Validation Accuracy: 0.9895\n",
      "\n",
      "Epoch 16:\n",
      "  Training Loss: 0.0365, Training Accuracy: 0.9914\n",
      "  Validation Loss: 0.0751, Validation Accuracy: 0.9891\n",
      "\n",
      "Epoch 17:\n",
      "  Training Loss: 0.0359, Training Accuracy: 0.9915\n",
      "  Validation Loss: 0.0875, Validation Accuracy: 0.9891\n",
      "\n",
      "Epoch 18:\n",
      "  Training Loss: 0.0355, Training Accuracy: 0.9917\n",
      "  Validation Loss: 0.0711, Validation Accuracy: 0.9891\n",
      "\n",
      "Epoch 19:\n",
      "  Training Loss: 0.0350, Training Accuracy: 0.9917\n",
      "  Validation Loss: 0.0891, Validation Accuracy: 0.9890\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20:\n",
      "  Training Loss: 0.0346, Training Accuracy: 0.9918\n",
      "  Validation Loss: 0.0914, Validation Accuracy: 0.9891\n",
      "Model training complete.\n",
      "Model saved successfully to 'trained_model.h5'.\n",
      "Training history saved successfully to 'training_history.pkl'.\n",
      "Evaluating the model on the test set...\n",
      "\u001b[1m4418/4418\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 376us/step - accuracy: 0.9901 - loss: 0.0800\n",
      "Test Loss: 0.0849\n",
      "Test Accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "# Section 3B: Train the Neural Network\n",
    "\n",
    "# Define the neural network\n",
    "print(\"Defining the neural network model...\")\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", input_shape=(X_train.shape[1],), name=\"Input_Layer\"),\n",
    "    tf.keras.layers.Dense(64, activation=\"relu\", name=\"Hidden_Layer_1\"),\n",
    "    tf.keras.layers.Dense(32, activation=\"relu\", name=\"Hidden_Layer_2\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\", name=\"Output_Layer\"),\n",
    "])\n",
    "print(\"Model defined successfully.\")\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "print(\"Compiling the model...\")\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "print(\"Model compiled successfully.\")\n",
    "\n",
    "# Define a custom callback for logging\n",
    "class TrainingLogger(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"\\nEpoch {epoch + 1}:\")\n",
    "        print(\n",
    "            f\"  Training Loss: {logs['loss']:.4f}, Training Accuracy: {logs['accuracy']:.4f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"  Validation Loss: {logs['val_loss']:.4f}, Validation Accuracy: {logs['val_accuracy']:.4f}\"\n",
    "        )\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "history = model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[TrainingLogger()],\n",
    "    verbose=0  # Suppress default verbose to use custom logging\n",
    ")\n",
    "print(\"Model training complete.\")\n",
    "\n",
    "# Evaluate the model\n",
    "model.save(\"trained_model.h5\")\n",
    "print(\"Model saved successfully to 'trained_model.h5'.\")\n",
    "\n",
    "# Save the training history\n",
    "with open(\"training_history.pkl\", \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(\"Training history saved successfully to 'training_history.pkl'.\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Evaluating the model on the test set...\")\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from 'trained_model.h5'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,512</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">33</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Input_Layer (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │        \u001b[38;5;34m16,512\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_1 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Hidden_Layer_2 (\u001b[38;5;33mDense\u001b[0m)          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m33\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,883</span> (105.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m26,883\u001b[0m (105.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">26,881</span> (105.00 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m26,881\u001b[0m (105.00 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training history loaded successfully from 'training_history.pkl'.\n",
      "Loaded Training History:\n",
      "accuracy: [0.9904242753982544, 0.9904308915138245, 0.9904308915138245, 0.9904308915138245, 0.9904220700263977]...\n",
      "loss: [0.05502021685242653, 0.051535654813051224, 0.04984559491276741, 0.048342760652303696, 0.04700388386845589]...\n",
      "val_accuracy: [0.9901944398880005, 0.9901944398880005, 0.9901944398880005, 0.9901944398880005, 0.9901944398880005]...\n",
      "val_loss: [0.0537579283118248, 0.052960436791181564, 0.05247677117586136, 0.052535321563482285, 0.05290927365422249]...\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model\n",
    "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
    "print(\"Model loaded successfully from 'trained_model.h5'.\")\n",
    "model.summary()\n",
    "\n",
    "# Load the training history\n",
    "with open(\"training_history.pkl\", \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "print(\"Training history loaded successfully from 'training_history.pkl'.\")\n",
    "\n",
    "# Print the loaded training history (optional)\n",
    "print(\"Loaded Training History:\")\n",
    "for key, values in history.items():\n",
    "    print(f\"{key}: {values[:5]}...\")  # Show first 5 values as a preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_node(model, G, current_node, target_node, visited, prediction_cache, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Predicting next node from current node: {current_node}\")\n",
    "\n",
    "    # Cache predictions to avoid redundant computations\n",
    "    if current_node not in prediction_cache:\n",
    "        target_features = G.nodes[target_node][\"features\"]\n",
    "        predictions = []\n",
    "\n",
    "        # Filter neighbors with valid features\n",
    "        neighbors = [\n",
    "            neighbor for neighbor in G.neighbors(current_node)\n",
    "            if \"features\" in G.nodes[neighbor] and neighbor not in visited\n",
    "        ]\n",
    "\n",
    "        # Check if the target node is a direct neighbor\n",
    "        if target_node in neighbors:\n",
    "            if debug:\n",
    "                print(f\"Target node {target_node} is a direct neighbor. Moving to target.\")\n",
    "            return target_node\n",
    "\n",
    "        # Predict probabilities for remaining neighbors\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_features = G.nodes[neighbor][\"features\"]\n",
    "            feature_vector = neighbor_features - target_features\n",
    "            prob = model.predict(feature_vector.reshape(1, -1), verbose=0)[0][0]  # Suppress model output\n",
    "            predictions.append((neighbor, prob))\n",
    "        \n",
    "        # Sort predictions by probability in descending order\n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        prediction_cache[current_node] = predictions\n",
    "\n",
    "    # Select the next node with the highest probability\n",
    "    next_node = None\n",
    "    for neighbor, prob in prediction_cache[current_node]:\n",
    "        if neighbor not in visited:\n",
    "            next_node = neighbor\n",
    "            break\n",
    "    \n",
    "    if debug:\n",
    "        print(f\"Next node chosen: {next_node}\")\n",
    "    return next_node\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Find the path with a limit on the number of hops\n",
    "def find_path(model, G, source, target, max_hops=40, debug=False):\n",
    "    if debug:\n",
    "        print(f\"Starting pathfinding from source: {source} to target: {target}, with max hops: {max_hops}\")\n",
    "    current_node = source\n",
    "    visited = set()\n",
    "    prediction_cache = {}  # Cache predictions to avoid recomputation\n",
    "    path = [source]\n",
    "    hops = 0\n",
    "\n",
    "    while current_node != target:\n",
    "        print(f\"Current Number of Hops: {hops}\")\n",
    "        print(f\"Current Node: {current_node}\")\n",
    "        visited.add(current_node)\n",
    "        next_node = predict_next_node(model, G, current_node, target, visited, prediction_cache, debug=debug)\n",
    "        if next_node is None:\n",
    "            if debug:\n",
    "                print(f\"Pathfinding failed: no valid neighbors from {current_node}.\")\n",
    "            return None  # No path found\n",
    "        path.append(next_node)\n",
    "        current_node = next_node\n",
    "        hops += 1\n",
    "\n",
    "        if hops > max_hops:\n",
    "            if debug:\n",
    "                print(f\"Pathfinding terminated: exceeded max hops ({max_hops}).\")\n",
    "            return None\n",
    "\n",
    "    if debug:\n",
    "        print(f\"Pathfinding complete. Path: {path}\")\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1/20: Source 22723 -> Target 6289\n",
      "Current Number of Hops: 0\n",
      "Current Node: 22723\n",
      "Current Number of Hops: 1\n",
      "Current Node: 3307\n",
      "Current Number of Hops: 2\n",
      "Current Node: 4597\n",
      "Current Number of Hops: 3\n",
      "Current Node: 3849\n",
      "Current Number of Hops: 4\n",
      "Current Node: 3339\n",
      "Current Number of Hops: 5\n",
      "Current Node: 9072\n",
      "Current Number of Hops: 6\n",
      "Current Node: 6891\n",
      "Current Number of Hops: 7\n",
      "Current Node: 8995\n",
      "Current Number of Hops: 8\n",
      "Current Node: 7078\n",
      "Current Number of Hops: 9\n",
      "Current Node: 8300\n",
      "Current Number of Hops: 10\n",
      "Current Node: 3476\n",
      "Current Number of Hops: 11\n",
      "Current Node: 3178\n",
      "Current Number of Hops: 12\n",
      "Current Node: 3310\n",
      "Current Number of Hops: 13\n",
      "Current Node: 17087\n",
      "Current Number of Hops: 14\n",
      "Current Node: 16410\n",
      "Current Number of Hops: 15\n",
      "Current Node: 10189\n",
      "Current Number of Hops: 16\n",
      "Current Node: 13585\n",
      "Current Number of Hops: 17\n",
      "Current Node: 801\n",
      "Current Number of Hops: 18\n",
      "Current Node: 806\n",
      "Current Number of Hops: 19\n",
      "Current Node: 22827\n",
      "Current Number of Hops: 20\n",
      "Current Node: 799\n",
      "  No path found or run terminated.\n",
      "Run 2/20: Source 12466 -> Target 27255\n",
      "Current Number of Hops: 0\n",
      "Current Node: 12466\n",
      "Current Number of Hops: 1\n",
      "Current Node: 12767\n",
      "Current Number of Hops: 2\n",
      "Current Node: 15869\n",
      "Current Number of Hops: 3\n",
      "Current Node: 10643\n",
      "Current Number of Hops: 4\n",
      "Current Node: 22929\n",
      "Current Number of Hops: 5\n",
      "Current Node: 15558\n",
      "Current Number of Hops: 6\n",
      "Current Node: 11786\n",
      "Current Number of Hops: 7\n",
      "Current Node: 13455\n",
      "Current Number of Hops: 8\n",
      "Current Node: 16545\n",
      "Current Number of Hops: 9\n",
      "Current Node: 10822\n",
      "Current Number of Hops: 10\n",
      "Current Node: 14161\n",
      "Current Number of Hops: 11\n",
      "Current Node: 27595\n",
      "Current Number of Hops: 12\n",
      "Current Node: 14942\n",
      "Current Number of Hops: 13\n",
      "Current Node: 28667\n",
      "Current Number of Hops: 14\n",
      "Current Node: 12851\n",
      "Current Number of Hops: 15\n",
      "Current Node: 11889\n",
      "Current Number of Hops: 16\n",
      "Current Node: 9640\n",
      "Current Number of Hops: 17\n",
      "Current Node: 10784\n",
      "Current Number of Hops: 18\n",
      "Current Node: 15994\n",
      "Current Number of Hops: 19\n",
      "Current Node: 13701\n",
      "Current Number of Hops: 20\n",
      "Current Node: 18837\n",
      "  No path found or run terminated.\n",
      "Run 3/20: Source 22807 -> Target 15756\n",
      "Current Number of Hops: 0\n",
      "Current Node: 22807\n",
      "  No path found or run terminated.\n",
      "Run 4/20: Source 26263 -> Target 13640\n",
      "Current Number of Hops: 0\n",
      "Current Node: 26263\n",
      "  No path found or run terminated.\n",
      "Run 5/20: Source 9127 -> Target 12594\n",
      "Current Number of Hops: 0\n",
      "Current Node: 9127\n",
      "  No path found or run terminated.\n",
      "Run 6/20: Source 5995 -> Target 19483\n",
      "Current Number of Hops: 0\n",
      "Current Node: 5995\n",
      "Current Number of Hops: 1\n",
      "Current Node: 8348\n",
      "Current Number of Hops: 2\n",
      "Current Node: 11378\n",
      "Current Number of Hops: 3\n",
      "Current Node: 12699\n",
      "Current Number of Hops: 4\n",
      "Current Node: 541\n",
      "Current Number of Hops: 5\n",
      "Current Node: 5410\n",
      "Current Number of Hops: 6\n",
      "Current Node: 8924\n",
      "Current Number of Hops: 7\n",
      "Current Node: 295\n",
      "Current Number of Hops: 8\n",
      "Current Node: 25489\n",
      "Current Number of Hops: 9\n",
      "Current Node: 22660\n",
      "Current Number of Hops: 10\n",
      "Current Node: 540\n",
      "Current Number of Hops: 11\n",
      "Current Node: 5700\n",
      "Current Number of Hops: 12\n",
      "Current Node: 581\n",
      "Current Number of Hops: 13\n",
      "Current Node: 16128\n",
      "Current Number of Hops: 14\n",
      "Current Node: 2001\n",
      "Current Number of Hops: 15\n",
      "Current Node: 4547\n",
      "Current Number of Hops: 16\n",
      "Current Node: 28389\n",
      "Current Number of Hops: 17\n",
      "Current Node: 13842\n",
      "Current Number of Hops: 18\n",
      "Current Node: 10823\n",
      "Current Number of Hops: 19\n",
      "Current Node: 20092\n",
      "Current Number of Hops: 20\n",
      "Current Node: 13178\n",
      "  No path found or run terminated.\n",
      "Run 7/20: Source 20186 -> Target 612\n",
      "Current Number of Hops: 0\n",
      "Current Node: 20186\n",
      "Current Number of Hops: 1\n",
      "Current Node: 21431\n",
      "Current Number of Hops: 2\n",
      "Current Node: 13178\n",
      "Current Number of Hops: 3\n",
      "Current Node: 1060\n",
      "Current Number of Hops: 4\n",
      "Current Node: 17346\n",
      "Current Number of Hops: 5\n",
      "Current Node: 1638\n",
      "Current Number of Hops: 6\n",
      "Current Node: 18505\n",
      "Current Number of Hops: 7\n",
      "Current Node: 10823\n",
      "Current Number of Hops: 8\n",
      "Current Node: 11891\n",
      "Current Number of Hops: 9\n",
      "Current Node: 541\n",
      "Current Number of Hops: 10\n",
      "Current Node: 6347\n",
      "Current Number of Hops: 11\n",
      "Current Node: 6106\n",
      "Current Number of Hops: 12\n",
      "Current Node: 6277\n",
      "Current Number of Hops: 13\n",
      "Current Node: 6034\n",
      "Current Number of Hops: 14\n",
      "Current Node: 4786\n",
      "Current Number of Hops: 15\n",
      "Current Node: 10000\n",
      "Current Number of Hops: 16\n",
      "Current Node: 13661\n",
      "Current Number of Hops: 17\n",
      "Current Node: 2998\n",
      "Current Number of Hops: 18\n",
      "Current Node: 4695\n",
      "Current Number of Hops: 19\n",
      "Current Node: 2999\n",
      "Current Number of Hops: 20\n",
      "Current Node: 22503\n",
      "  No path found or run terminated.\n",
      "Run 8/20: Source 15598 -> Target 14075\n",
      "Current Number of Hops: 0\n",
      "Current Node: 15598\n",
      "Current Number of Hops: 1\n",
      "Current Node: 10383\n",
      "Current Number of Hops: 2\n",
      "Current Node: 8410\n",
      "Current Number of Hops: 3\n",
      "Current Node: 12092\n",
      "Current Number of Hops: 4\n",
      "Current Node: 12851\n",
      "Current Number of Hops: 5\n",
      "Current Node: 10643\n",
      "Current Number of Hops: 6\n",
      "Current Node: 1539\n",
      "Current Number of Hops: 7\n",
      "Current Node: 5363\n",
      "Current Number of Hops: 8\n",
      "Current Node: 12847\n",
      "Current Number of Hops: 9\n",
      "Current Node: 16558\n",
      "Current Number of Hops: 10\n",
      "Current Node: 10590\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m success_rate, average_hops\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m success_rate, average_hops \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pathfinding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hops\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 18\u001b[0m, in \u001b[0;36mevaluate_pathfinding\u001b[0;34m(model, G, max_hops, num_runs)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_runs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: Source \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource_node\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> Target \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_node\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Call the pathfinding function\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[43mfind_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_hops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path:\n\u001b[1;32m     21\u001b[0m     num_hops \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(path) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Number of hops is the length of the path minus 1\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 53\u001b[0m, in \u001b[0;36mfind_path\u001b[0;34m(model, G, source, target, max_hops, debug)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Node: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent_node\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m visited\u001b[38;5;241m.\u001b[39madd(current_node)\n\u001b[0;32m---> 53\u001b[0m next_node \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_next_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mG\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_node\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisited\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m next_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m debug:\n",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m, in \u001b[0;36mpredict_next_node\u001b[0;34m(model, G, current_node, target_node, visited, prediction_cache, debug)\u001b[0m\n\u001b[1;32m     17\u001b[0m     neighbor_features \u001b[38;5;241m=\u001b[39m G\u001b[38;5;241m.\u001b[39mnodes[neighbor][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     18\u001b[0m     feature_vector \u001b[38;5;241m=\u001b[39m neighbor_features \u001b[38;5;241m-\u001b[39m target_features\n\u001b[0;32m---> 19\u001b[0m     prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_vector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Suppress model output\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     predictions\u001b[38;5;241m.\u001b[39mappend((neighbor, prob))\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Sort predictions by probability in descending order\u001b[39;00m\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:448\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28mself\u001b[39m, x, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    446\u001b[0m ):\n\u001b[1;32m    447\u001b[0m     \u001b[38;5;66;03m# Create an iterator that yields batches of input data.\u001b[39;00m\n\u001b[0;32m--> 448\u001b[0m     epoch_iterator \u001b[38;5;241m=\u001b[39m \u001b[43mTFEpochIterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    449\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_per_execution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msteps_per_execution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;66;03m# Container that configures and calls callbacks.\u001b[39;00m\n\u001b[1;32m    458\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(callbacks, callbacks_module\u001b[38;5;241m.\u001b[39mCallbackList):\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:666\u001b[0m, in \u001b[0;36mTFEpochIterator.__init__\u001b[0;34m(self, distribute_strategy, *args, **kwargs)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy \u001b[38;5;241m=\u001b[39m distribute_strategy\n\u001b[0;32m--> 666\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataset, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedDataset):\n\u001b[1;32m    668\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_distribute_strategy\u001b[38;5;241m.\u001b[39mexperimental_distribute_dataset(\n\u001b[1;32m    669\u001b[0m         dataset\n\u001b[1;32m    670\u001b[0m     )\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:675\u001b[0m, in \u001b[0;36mTFEpochIterator._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_iterator\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 675\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_tf_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:236\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     indices_dataset \u001b[38;5;241m=\u001b[39m indices_dataset\u001b[38;5;241m.\u001b[39mmap(tf\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mshuffle)\n\u001b[0;32m--> 236\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mslice_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindices_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m options \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mOptions()\n\u001b[1;32m    239\u001b[0m options\u001b[38;5;241m.\u001b[39mexperimental_distribute\u001b[38;5;241m.\u001b[39mauto_shard_policy \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    240\u001b[0m     tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mAutoShardPolicy\u001b[38;5;241m.\u001b[39mDATA\n\u001b[1;32m    241\u001b[0m )\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/keras/src/trainers/data_adapters/array_data_adapter.py:197\u001b[0m, in \u001b[0;36mArrayDataAdapter.get_tf_dataset.<locals>.slice_inputs\u001b[0;34m(indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    191\u001b[0m inputs \u001b[38;5;241m=\u001b[39m array_slicing\u001b[38;5;241m.\u001b[39mconvert_to_sliceable(\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inputs, target_backend\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m )\n\u001b[1;32m    194\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mlists_to_tuples(inputs)\n\u001b[1;32m    196\u001b[0m dataset \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset\u001b[38;5;241m.\u001b[39mzip(\n\u001b[0;32m--> 197\u001b[0m     (indices_dataset, \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrepeat())\n\u001b[1;32m    198\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_batch\u001b[39m(i, data):\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrab_one\u001b[39m(x):\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/dataset_ops.py:741\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[0;32m--> 741\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensors_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensors_op.py:23\u001b[0m, in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/tensorflow/python/data/ops/from_tensors_op.py:35\u001b[0m, in \u001b[0;36m_TensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m---> 35\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_flat_tensor_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_structure\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[0;32m~/cs6850/Final_Project/.venv/lib/python3.12/site-packages/tensorflow/python/ops/gen_dataset_ops.py:7711\u001b[0m, in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   7710\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 7711\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7712\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTensorDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7713\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7714\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   7715\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def evaluate_pathfinding(model, G, max_hops=20, num_runs=20):\n",
    "    total_hops = 0\n",
    "    successful_runs = 0\n",
    "\n",
    "    # Filter nodes with features\n",
    "    nodes_with_features = [node for node in G.nodes if \"features\" in G.nodes[node]]\n",
    "\n",
    "    for run in range(num_runs):\n",
    "        # Ensure start and target nodes have features\n",
    "        if len(nodes_with_features) < 2:\n",
    "            print(\"Not enough nodes with features for evaluation.\")\n",
    "            break\n",
    "\n",
    "        source_node, target_node = random.sample(nodes_with_features, 2)\n",
    "        print(f\"Run {run + 1}/{num_runs}: Source {source_node} -> Target {target_node}\")\n",
    "        \n",
    "        # Call the pathfinding function\n",
    "        path = find_path(model, G, source_node, target_node, max_hops)\n",
    "        \n",
    "        if path:\n",
    "            num_hops = len(path) - 1  # Number of hops is the length of the path minus 1\n",
    "            print(f\"  Path found in {num_hops} hops. Path: {path}\")\n",
    "            total_hops += num_hops\n",
    "            successful_runs += 1\n",
    "        else:\n",
    "            print(\"  No path found or run terminated.\")\n",
    "    \n",
    "    if successful_runs > 0:\n",
    "        average_hops = total_hops / successful_runs\n",
    "        success_rate = successful_runs / num_runs * 100\n",
    "    else:\n",
    "        average_hops = float('inf')  # No successful runs\n",
    "        success_rate = 0.0\n",
    "    \n",
    "    print(f\"\\n--- Summary ---\")\n",
    "    print(f\"Success rate: {success_rate:.2f}% ({successful_runs}/{num_runs})\")\n",
    "    print(f\"Average hops: {average_hops:.2f}\" if successful_runs > 0 else \"No successful runs.\")\n",
    "    \n",
    "    return success_rate, average_hops\n",
    "\n",
    "\n",
    "# Run evaluation\n",
    "success_rate, average_hops = evaluate_pathfinding(model, G, max_hops=20, num_runs=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
